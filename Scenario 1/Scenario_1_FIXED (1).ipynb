{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQZNn6wI5jo8",
    "outputId": "c9f92cc3-9104-4ab4-b86d-7c23572d93c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dotenv in /usr/local/lib/python3.11/dist-packages (0.9.9)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from dotenv) (1.1.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.46.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
      "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.0)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.43.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.6.15)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K\n",
      "up to date, audited 23 packages in 755ms\n",
      "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K\n",
      "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K3 packages are looking for funding\n",
      "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K  run `npm fund` for details\n",
      "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K\n",
      "2 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m severity vulnerabilities\n",
      "\n",
      "To address all issues (including breaking changes), run:\n",
      "  npm audit fix --force\n",
      "\n",
      "Run `npm audit` for details.\n",
      "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K"
     ]
    }
   ],
   "source": [
    "!pip install dotenv\n",
    "!pip install transformers accelerate\n",
    "!pip install streamlit\n",
    "!npm install localtunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xN3CQ_i95sI2",
    "outputId": "c8963105-30ac-4ae3-c301-55fb2e0a1adb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting CodeModel.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile CodeModel.py\n",
    "\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from accelerate import init_empty_weights, infer_auto_device_map, load_checkpoint_in_model\n",
    "import torch\n",
    "import time\n",
    "from huggingface_hub import login\n",
    "\n",
    "# (replace with your hugging face token)\n",
    "login(\"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\")\n",
    "\n",
    "def generate_code(prompt):\n",
    "    model_id = \"Salesforce/codegen-350M-multi\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "    with init_empty_weights():\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "    device_map = infer_auto_device_map(model, no_split_module_classes=[\"LlamaDecoderLayer\"],\n",
    "                                       max_memory={\"cpu\": \"48GiB\"})\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=device_map,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=device_map,\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    sequences = pipe(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        temperature=0.1,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=128\n",
    "    )\n",
    "    result = ''.join([s['generated_text'] for s in sequences])\n",
    "    print(result)\n",
    "    end = time.time()\n",
    "    return result, start, end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "616ebc0592b4435eb2a267f2dffb586a",
      "529c4703ed864bb3a42f69feb3c13c34",
      "9d60681469bf4ebabcd880e6507047c4",
      "9b14c7537fbc423e935394c0ebd77a02",
      "05b8e38f47a44826b99626d79a39158f",
      "a5f48649fc934e2d9265b6eaf34b7a14",
      "3bf75c05894d4bafbc3ce5aa78b3d9c8",
      "b8d1171d90bd48eb831adb35d6c0cddc",
      "2fa4121de2994de3b47876052802f4a5",
      "1bd6f994dfee4587a1e7bfea96440602",
      "449a375c8a7f483eb99f5c70cc294449",
      "b805c0da2e544775bbe4b160a37040dd",
      "cf64d849e6cb4e268ec7079d0a6d45ef",
      "41a842d5033b4b33af4fe1f77f1518e4",
      "e7ba95ccbe14462387f352050aa70111",
      "9dab1ddba3344d3aa0087a6991a3d780",
      "c784274893af4ba9ba9827d491079e27",
      "b085806088dd4ad5a76e6c97a10af297",
      "b53b0a0212a9452cb3cd2e9c2fffcb66",
      "291d3fea1c194198b11efe78f461c7a0",
      "5e39f3cad03240639d405e681ffe84de",
      "88c073f5543443df8d1850cbf940ff11",
      "ce96b0dd334d4a699365d48fe43d6742",
      "6bbcd8ad42ae44edb520ffe547ec1e43",
      "10b55794d777485fb8e77774d63212d4",
      "06ed402f46914eaca36b6d671343bdeb",
      "89e34d2b7c7b4554b81e69c275871c41",
      "90548d6809914f748adb761c17f88f4e",
      "5f08b2b988d845e8928f142a00509904",
      "3bc3797a189448e88958ed99f008e5cb",
      "fcf4e7af6f66438d9c44eede1ec35494",
      "7cd2cc3a26da466d837d55fe45dfdc10",
      "33c934ccd79f40cab30d5ad2c8c0a521",
      "9544530198b04c6a800844f8538d2821",
      "1d39aef50f6949369ddc16b8524515b6",
      "54091bb1081f42feafdf8367a741d769",
      "ab6da1dc903d40d9a4ccb0fbae5c3a7c",
      "892e6197b5e54dee86ca6bf154779a15",
      "7a17e197769d4b8abb2fa394690e5052",
      "6c1e610e2cdd45a98e17779033526ba9",
      "941a5d14f4944620b8b82d2dc23391f4",
      "29953f929e0641559685c065b8edfd7c",
      "58166367021a438c919f7c028e33d59c",
      "5b64e4db80624882925976f1ac1c2434",
      "ea62c521fec34a18a85517b9f2a04282",
      "55a0f37e8d014b2e9a4a91fc09dcc182",
      "7d3a83e1e5804628af3bc3a62675a665",
      "21da3feb848f471d83e0ae19b9f13790",
      "f629858a1b5b41d2b3db55a89585f524",
      "e569dd4d5c19436b94d703b5c98aa286",
      "fbe8a2263eb6472793c640b42802fd15",
      "5fe38b60e3f14bd9a497eb169c0b1b4b",
      "9b477977fca143e8ad615dc1247353eb",
      "00599b2b668e45baa26071abb3d4382d",
      "a58f2c2e66e64fcc9671404ccc462433",
      "14968a70529e4307b4456460da1ff93b",
      "0850ee0cb4924e6ea170d1502a18f67d",
      "b9cc411df030460c944bbf96dcf3185c",
      "54f2d2104c5b442e91ae1b1c4d4e0cc4",
      "5d499f3c5e5744759b77ef62e18c4eea",
      "2bcf7da95e6b4199b3c21e7c46d54144",
      "9a70831bed554e169562407fca4d99c7",
      "63cdc96e70da4753a6bcfc012aab17df",
      "ae323d1093df429a8090e562a832bed8",
      "baceeb94b88a4d74ab70aec3313a202b",
      "e3e29216c706458eb657cc8ba086397a",
      "aeb01a326adc475e8f41d0141fde4e39",
      "a3fa93d303e44dc18bcf1f063ab5e644",
      "4b42b966735548cc826d35c44bc5e9d2",
      "a6a4b5fb16f84e11933dd2b11f7fe384",
      "762304090e0b42e5bfa42d73c28b4faa",
      "144cb1ad13da49feb5c924cd579538ad",
      "2ebbc33a4c4e4238800c0ba8652ff48f",
      "622ccca0db674ce684085ddec790943f",
      "d2785fbef1874b7792531feb9f1dd04e",
      "799b1ac553774a0db872bb648fcba21a",
      "46d3f37c3b5947afb51dc15372a61a1b",
      "66d8ec26e2704f92967f47caa039251e",
      "71e9c890738c44b1a4eed9b42eb6a03a",
      "f5cd301fd44c4ec19d3a88773a7d7213",
      "1e0c58fae28b4f6fb76eb3ce5af713f4",
      "051a48460e474523b6f6340cf6aa9048",
      "3b416ff79f1e4134b82c0472abba3d3c",
      "4e9d07bfd7fe4b8289a5ae35d8e0000e",
      "012bc757ef4c48bc936dc4168d17ff7b",
      "3f781ca969b641509bbfa2d1cdd4ba79",
      "fa3d437fe2a04fbbb0a95b4f53952bc6",
      "28a867cfb02c4b2aa24e10140d8e8a72",
      "c4831188a1e64c458b53b25c8a1e5fbc",
      "fe800031239244bf9947d15727c8f1e0",
      "a4940ceced194293bb5ba2dee0c7e6b3",
      "f1c1f72f6c634380a1bd2a62bffb94e8",
      "fd31487085c34ca6803b711e8b70d821",
      "3e58c8f7beda4ad089ded43b053772ba",
      "d2e4c2d8ea554943a29af140602bbd68",
      "650eb0cde178488c92c9eb3335d599b6",
      "92112e23b8f14d8ba4c463469acf9098",
      "807a431294b84b23b6ee6be9db4686c2",
      "c330df2ca7b8401498f503cad3a2eaf5"
     ]
    },
    "id": "1qCaaAWl6orS",
    "outputId": "ed670270-0956-468f-abea-294cf3b774f1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616ebc0592b4435eb2a267f2dffb586a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/240 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b805c0da2e544775bbe4b160a37040dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce96b0dd334d4a699365d48fe43d6742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9544530198b04c6a800844f8538d2821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea62c521fec34a18a85517b9f2a04282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14968a70529e4307b4456460da1ff93b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb01a326adc475e8f41d0141fde4e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d8ec26e2704f92967f47caa039251e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/797M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4831188a1e64c458b53b25c8a1e5fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/797M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Salesforce/codegen-350M-multi were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
      "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at Salesforce/codegen-350M-multi were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
      "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fibonacci sequence\n",
      "    fib = fibonacci(n)\n",
      "    for i in range(n):\n",
      "        fib[i] = fib[i] + fib[i-1]\n",
      "    return fib\n",
      "\n",
      "def fibonacci(n):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    elif n == 2:\n",
      "        return fibonacci(n-1)\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "def fibonacci_seq(n):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    elif n == 2:\n",
      "        return fibonacci_seq(n-1)\n",
      "    else:\n",
      "        return fibonacci_seq(n-1) + fibonacci_seq(n-2)\n",
      "\n",
      "def fibonacci_seq_seq(n):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    elif n == 2:\n",
      "        return fibonacci_seq_seq(\n",
      "('fibonacci sequence\\n    fib = fibonacci(n)\\n    for i in range(n):\\n        fib[i] = fib[i] + fib[i-1]\\n    return fib\\n\\ndef fibonacci(n):\\n    if n == 0:\\n        return 0\\n    elif n == 1:\\n        return 1\\n    elif n == 2:\\n        return fibonacci(n-1)\\n    else:\\n        return fibonacci(n-1) + fibonacci(n-2)\\n\\ndef fibonacci_seq(n):\\n    if n == 0:\\n        return 0\\n    elif n == 1:\\n        return 1\\n    elif n == 2:\\n        return fibonacci_seq(n-1)\\n    else:\\n        return fibonacci_seq(n-1) + fibonacci_seq(n-2)\\n\\ndef fibonacci_seq_seq(n):\\n    if n == 0:\\n        return 0\\n    elif n == 1:\\n        return 1\\n    elif n == 2:\\n        return fibonacci_seq_seq(', 1750348810.7934208, 1750348857.9396136)\n"
     ]
    }
   ],
   "source": [
    "from CodeModel import generate_code\n",
    "print(generate_code('fibonacci sequence'), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mg_pcqgd6pH0",
    "outputId": "b1e03a8d-da26-424b-a553-05826e9c028c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import streamlit as st\n",
    "import time\n",
    "from CodeModel import generate_code\n",
    "\n",
    "st.header(\"CodeGenie: AI-powered code generator\")\n",
    "\n",
    "with st.form(\"my_form\"):\n",
    "    user_input = st.text_area(\"Enter your text prompt below and click the button to submit.\")\n",
    "    submit = st.form_submit_button(label=\"Submit text prompt\")\n",
    "\n",
    "if submit:\n",
    "    with st.spinner(text=\"Generating code... It may take some time\"):\n",
    "        code, start, end = generate_code(prompt=user_input)\n",
    "        hours, rem = divmod(end - start, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "        st.success(\n",
    "            \"Processing time: {:0>2}:{:0>2}:{:05.2f}.\".format(\n",
    "                int(hours), int(minutes), seconds\n",
    "            )\n",
    "        )\n",
    "        st.code(code, language='python')\n",
    "\n",
    "    st.sidebar.markdown(\"## Guide\")\n",
    "    st.sidebar.info(\"This tool uses Salesforce CodeGen 350M model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J38O0VP27C0R",
    "outputId": "48e890ac-63cb-4367-ab44-2af8bdc4bf33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.127.53.224\n"
     ]
    }
   ],
   "source": [
    "!wget -q -O - ipv4.icanhazip.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16ukdUc27DUc",
    "outputId": "cf453ef8-d3d1-4a94-90f8-1f9a798eb422"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.127.53.224\n",
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
      "\u001b[0m\n",
      "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.127.53.224:8501\u001b[0m\n",
      "\u001b[0m\n",
      "your url is: https://six-teams-begin.loca.lt\n",
      "2025-06-19 16:03:39.493751: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750349019.580904   12377 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750349019.602934   12377 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Some weights of the model checkpoint at Salesforce/codegen-350M-multi were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
      "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at Salesforce/codegen-350M-multi were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
      "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "write a code for displaying all prime numbers from 1 to 100\n",
      "    for i in range(1, 100):\n",
      "        print(i)\n",
      "\n",
      "# print the code for displaying all prime numbers from 1 to 10\n",
      "for i in range(1, 10):\n",
      "    print(i)\n",
      "\n",
      "# print the code for displaying all prime numbers from 1 to 10\n",
      "for i in range(1, 10):\n",
      "    print(i)\n",
      "\n",
      "# print the code for displaying all prime numbers from 1 to 10\n",
      "for i in range(1, 10):\n",
      "    print(i)\n",
      "\n",
      "# print the code for displaying all prime numbers from 1 to 10\n",
      "for i in range(1, 10):\n",
      "    print(i)\n",
      "\n",
      "# print the code for displaying all prime numbers from 1 to 10\n",
      "for i in range(1, 10):\n",
      "    print(i)\n",
      "\n",
      "# print the code for displaying all prime numbers from 1 to 10\n",
      "for i in range(1, 10):\n",
      "    print(i)\n",
      "\n",
      "# print the code for displaying all prime numbers from 1 to 10\n",
      "for i in range(1, 10):\n",
      "    print(i)\n",
      "\n",
      "# print the code for displaying all prime numbers from 1 to 10\n",
      "for i in range(1\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit -q\n",
    "!wget -q -O - ipv4.icanhazip.com\n",
    "\n",
    "!streamlit run app.py & npx localtunnel --port 8501\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlJ7Kl6d7FfM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
